import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm
import faiss
import streamlit as st
from kiwipiepy import Kiwi
from rank_bm25 import BM25Okapi
import google.generativeai as genai
import time




# # ê²½ë¡œ ì„¤ì •
# data_path = './data'
# module_path = './modules'

# Gemini ì„¤ì •
import google.generativeai as genai

# import shutil
# os.makedirs("/root/.streamlit", exist_ok=True)
# shutil.copy("secrets.toml", "/root/.streamlit/secrets.toml")

# GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
# genai.configure(api_key=GOOGLE_API_KEY)
genai.configure(api_key="key")  # API í‚¤ë¥¼ ì§ì ‘ ì…ë ¥
model = genai.GenerativeModel("gemini-1.5-flash")

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
faiss_index_path = "9000_faiss_index_path"
document_ids_path = "9000_chunk_document_ids_path"
file_path = '9000_chunk_sentence_modify_v18_result_final.csv'
user_dictionary_path = 'user_dictionary.txt'  # ì‚¬ìš©ì ì‚¬ì „ íŒŒì¼ ê²½ë¡œ

# # CSV íŒŒì¼ ë¡œë“œ
# csv_file_path = os.path.join(data_path, "jeju_final_sentences_v7_modify.csv")
# df = pd.read_csv(csv_file_path)

####------------UI-------------###
# Streamlit App UI
st.set_page_config(layout="wide", page_title="ğŸ½ï¸ì¢€ ìƒ‰ë‹¤ë¥¸ ë§›ì§‘ì„ ì°¾ëŠ”ë‹¤êµ¬? ìš°ë¦¬ê°€ í•´ê²°í•´ì¤„ê²Œ!ğŸ½ï¸")

import streamlit as st

# ------------------------ì „ì²´ ë°°ê²½/ê¸€ê¼´------------------------
st.markdown(
    """
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Jua&display=swap');
    * {
        font-family: 'Jua', sans-serif;
    }

    .stApp {
        background-color: #fdd8b3;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ------------------------íƒ€ì´í‹€------------------------
st.markdown(
    """
    <h1 style='font-family: "Jua", sans-serif;'>ğŸŠ ë‚˜ë§Œì˜ ì œì£¼ë„ ë§›ì§‘ ì—¬í–‰ ğŸŠ</h1>
    """,
    unsafe_allow_html=True
)

# ------------------------ì•± ì†Œê°œ------------------------
with st.expander('ë‹¤ë“¤ ëª¨ë¥´ëŠ” ê·¸ ê³³, ê¶ê¸ˆí•˜ì§€ ì•Šìœ¼ì„¸ìš”? ğŸ‘€'):
    st.markdown(
    """
    <style>
    .stExpander {
        background-color: #faded0;
        padding: 5px;  /* íŒ¨ë”© ì¶”ê°€ */
        border-radius: 10px;  /* ëª¨ì„œë¦¬ë¥¼ ë‘¥ê¸€ê²Œ */
    }
    </style>
    """,
    unsafe_allow_html=True
    )
    st.markdown(
    """
    <h3 style='font-family: "Jua", sans-serif;'>ë‚¨ë“¤ì€ ì •ë§ ìš´ì´ ì¢‹ì€ ê±¸ê¹Œìš”? ë‚˜ëŠ” ì™œ ë§¤ë²ˆ ì´ë ‡ê²Œ ê¸´ ì›¨ì´íŒ…ì— ì§€ì¹ ê¹Œìš”?</h3>
    """,
    unsafe_allow_html=True
    )
    st.write('ê¸°ëŒ€ê°ì„ ì•ˆê³  ì–´ë µê²Œ í•œ ì… ë§›ë´¤ì§€ë§Œ, ê·¸ì € ê·¸ëŸ° ë§›ì´ì—ˆì„ ë•Œ... ëª¨ë‘ ì´ëŸ¬í•œ ê²½í—˜ì´ ìˆì§€ ì•Šìœ¼ì‹ ê°€ìš”? ğŸ˜¢')
    st.write('ì €í¬ ì•±ì€ ê·¸ëŸ° ì‹¤ë§ìŠ¤ëŸ¬ìš´ ê²½í—˜ ëŒ€ì‹ , **ğŸ’ìˆ¨ê²¨ì§„ ë³´ì„ ê°™ì€ ë§›ì§‘ğŸ’** ì„ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.')
    st.write('ìœ ëª…í•œ ìŒì‹ì ë“¤ì— ê°€ë ¤ì ¸ ìˆë˜ **ì œì£¼ë„ì˜ ì§„ì§œ ë§›ì§‘**, ê·¸ê³³ì„ ì €í¬ê°€ ì•Œë ¤ë“œë¦´ê²Œìš”! ğŸ½ï¸')

@st.cache_data
def load_data():
    data = pd.read_csv(file_path)
    return data

@st.cache_data
# ë°ì´í„° ë° ì¸ë±ìŠ¤ ë¡œë“œ
def load_data_and_index():
    if "data_loaded" not in st.session_state:
        # CSV íŒŒì¼ ë¡œë“œ ë° chunk_texts ìƒì„±
        data = pd.read_csv(file_path)
        chunk_texts = []
        
        for idx, row in data.iterrows():
            text_chunks = []
            for col in ["ì—…ì¢…ëª…", "ì£¼ì†Œ", "ê¸°ì¤€ì—°ì›”", "ì´ìš©ê±´ìˆ˜êµ¬ê°„", 'ì´ìš©ê¸ˆì•¡êµ¬ê°„', "ìš”ì¼ë³„", "ì‹œê°„ëŒ€ë³„", "ì„±ë³„", "ì—°ë ¹ë³„", "í˜„ì§€ì¸", 'url']:
                chunk_text = row[col]
                if pd.notna(chunk_text):
                    text_chunks.append(chunk_text)
            chunk_texts.append(" ".join(text_chunks))
        st.session_state.chunk_texts = chunk_texts
        print("chunk_texts ìƒì„± ì™„ë£Œ")
        
        # document_ids ë¡œë“œ
        st.session_state.chunk_document_ids = np.load(document_ids_path, allow_pickle=True).tolist()
        print("Document IDs ë¡œë“œ ì™„ë£Œ")
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        st.session_state.index = faiss.read_index(faiss_index_path)
        print("FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        
        # Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ì™€ ì‚¬ìš©ì ì‚¬ì „ ë¡œë“œ
        kiwi = Kiwi()
        added_words_count = kiwi.load_user_dictionary(user_dictionary_path)
        print(f"ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì—ì„œ {added_words_count}ê°œì˜ ë‹¨ì–´ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.session_state.kiwi = kiwi
        
        # BM25ì— ì‚¬ìš©í•  ì½”í¼ìŠ¤ í† í°í™” ë° BM25 ëª¨ë¸ ìƒì„±
        tokenized_corpus = [[token.form for token in st.session_state.kiwi.tokenize(doc)] for doc in st.session_state.chunk_texts]
        st.session_state.bm25 = BM25Okapi(tokenized_corpus)
        print("BM25 ëª¨ë¸ ìƒì„± ì™„ë£Œ")

        # BGE í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ì„¤ì • (CPUì—ì„œ ì‹¤í–‰)
        try:
            st.session_state.tokenizer_bge = AutoTokenizer.from_pretrained("upskyy/bge-m3-korean")
            st.session_state.model_bge = AutoModel.from_pretrained("upskyy/bge-m3-korean")  # .cuda() ì œê±°
            print("BGE í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            print(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜ˆì™¸ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€
        
        st.session_state.data_loaded = True  # ë°ì´í„°ê°€ ë¡œë“œëœ ìƒíƒœë¡œ í”Œë˜ê·¸ ë³€ê²½
    else:
        print("ë°ì´í„° ë° ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

# BM25 + FAISS ë³‘ë ¬ ê²€ìƒ‰ í•¨ìˆ˜ (ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ ë°ì´í„° í™œìš©)
def search_documents(query, bm25_k=1000, faiss_k=10):
    # 1. Kiwië¥¼ ì´ìš©í•œ BM25 1ì°¨ ê²€ìƒ‰
    tokenized_query_kiwi = [token.form for token in st.session_state.kiwi.tokenize(query)]
    bm25_results = st.session_state.bm25.get_top_n(tokenized_query_kiwi, st.session_state.chunk_texts, n=bm25_k)
    
    # 2. BM25 í•„í„°ë§ëœ ë¬¸ì„œë“¤ì˜ document_id ì¶”ì¶œ
    bm25_filtered_ids = [st.session_state.chunk_document_ids[st.session_state.chunk_texts.index(doc)] for doc in bm25_results]
    
    # 3. BGE í† í¬ë‚˜ì´ì €ë¡œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (CPUì—ì„œ ì‹¤í–‰)
    query_inputs_bge = st.session_state.tokenizer_bge(query, return_tensors="pt", padding=True, truncation=True)  # .to('cuda') ì œê±°
    with torch.no_grad():
        query_outputs_bge = st.session_state.model_bge(**query_inputs_bge)
        query_embedding_bge = query_outputs_bge.last_hidden_state.mean(dim=1).cpu().numpy()  # GPU ê´€ë ¨ ì½”ë“œ ì œê±°
    
    # Normalize query embedding for cosine similarity
    faiss.normalize_L2(query_embedding_bge)
    
    # FAISS ê²€ìƒ‰ ìˆ˜í–‰
    distances, indices = st.session_state.index.search(query_embedding_bge, faiss_k)
    print(f"FAISS ê²€ìƒ‰ ì™„ë£Œ - ìƒìœ„ {faiss_k}ê°œì˜ ê²€ìƒ‰ ê²°ê³¼:")
    
    # FAISSì—ì„œ ì°¾ì€ ìƒìœ„ ë¬¸ì„œë“¤ì˜ document_id ì¶”ì¶œ
    retrieved_document_ids = [st.session_state.chunk_document_ids[idx] for idx in indices[0]]
    unique_document_ids = list(dict.fromkeys(retrieved_document_ids))  # ì¤‘ë³µ ì œê±°
    
    # ìµœì¢… ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ ë°˜í™˜
    retrieved_documents = [st.session_state.chunk_texts[doc_id] for doc_id in unique_document_ids]
    
    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥
    for i, doc in enumerate(retrieved_documents, 1):
        print(f"Document {i}: {doc}")

    return retrieved_documents

# LLM(Gemini) ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_gemini_response(query, retrieved_documents):
    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í”„ë¡¬í”„íŠ¸ë¡œ ê²°í•©
    context = " ".join(retrieved_documents)
    
    # ëª¨ë¸ì— ì „ë‹¬í•  ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = (
        f"As a assistant helping with tourism in Jeju Island, "
        f"please refer to the following information:\n{context}\n\n"
        f"Based on this information, please provide a response to the following query:\n'{query}'"
    )
    
    # Gemini ëª¨ë¸ ì´ˆê¸°í™” ë° ì‘ë‹µ ìƒì„±
    gemini_model = genai.GenerativeModel("gemini-1.5-flash")
    response = gemini_model.generate_content(prompt)
    return response.text

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ ë°ì´í„° ë¡œë“œ)
if 'messages' not in st.session_state:
    st.session_state.messages = []

# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ í˜¸ì¶œ (ìµœì´ˆ ì‹¤í–‰ ì‹œì—ë§Œ)
load_data_and_index()

# ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬ë¬¸ (st.text_inputìœ¼ë¡œ ìˆ˜ì •)
prompt = st.text_input("ìœ„ì¹˜ì™€ ìŒì‹ ì¢…ë¥˜ë¥¼ ì…ë ¥í•´ë³´ì„¸ìš”", "")

# ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€ ì²˜ë¦¬
if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    # st.write(f"User: {prompt}")

    # Generate a new response if last message is not from assistant
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.spinner("ë§›ì§‘ì„ ì°¾ê³  ìˆì–´ìš”! ğŸœğŸ²ğŸ—ğŸ£ğŸ¥¯"):
            # 1. BM25 + FAISS ë³‘ë ¬ ê²€ìƒ‰ ìˆ˜í–‰
            retrieved_documents = search_documents(prompt)
            
            # 2. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM(Gemini) ì‘ë‹µ ìƒì„±
            full_response = generate_gemini_response(prompt, retrieved_documents)

            placeholder = st.empty()
            placeholder.markdown(full_response)
        
        message = {"role": "assistant", "content": full_response}
        st.session_state.messages.append(message)
